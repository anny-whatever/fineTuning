{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098075de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Optimal sequence length for medical prescriptions (short responses)\n",
    "max_seq_length = 1024\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"google/gemma-2-9b-it\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,  # Auto-detect best dtype\n",
    "    load_in_4bit=True,  # 4-bit quantization for efficiency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38087afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,  # LoRA rank - balanced for medical precision\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,  # Scaling factor (typically = r)\n",
    "    lora_dropout = 0,  # 0 is optimized for Unsloth\n",
    "    bias = \"none\",  # \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\",  # 30% less VRAM\n",
    "    random_state = 42,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d70a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma\",  # Gemma-2 uses \"gemma\" template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4912202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clinic chatbot training data\n",
    "data_path = os.path.join(os.path.dirname(os.getcwd()), 'data', 'clinic_chatbot_training_data.json')\n",
    "with open(data_path, 'r') as f:\n",
    "    clinic_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(clinic_data)} training examples\")\n",
    "print(\"Sample:\", clinic_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0fc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add \"speech\" field to doctor_output - natural language explanation\n",
    "def add_speech_field(item):\n",
    "    \"\"\"Convert structured prescription into natural language speech\"\"\"\n",
    "    doc_out = item['doctor_output']\n",
    "    \n",
    "    # Generate natural speech from prescription\n",
    "    speech = (\n",
    "        f\"{doc_out['prescription_text']} \"\n",
    "        f\"I'm prescribing {doc_out['medicine_name']}. \"\n",
    "        f\"Please take {doc_out['dose_size']} {doc_out['frequency'].lower()} \"\n",
    "        f\"for {doc_out['duration']}. \"\n",
    "        f\"Make sure to follow the dosage instructions carefully and contact me if symptoms persist or worsen.\"\n",
    "    )\n",
    "    \n",
    "    # Add speech field to doctor_output\n",
    "    doc_out['speech'] = speech\n",
    "    \n",
    "    return {\n",
    "        \"patient_input\": item['patient_input'],\n",
    "        \"doctor_output\": doc_out\n",
    "    }\n",
    "\n",
    "# Process all data to add speech field\n",
    "processed_data = [add_speech_field(item) for item in clinic_data]\n",
    "print(\"Sample with speech field:\", processed_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to chat format for training\n",
    "def convert_to_chat_format(item):\n",
    "    \"\"\"Convert clinic data to chat conversation format\"\"\"\n",
    "    \n",
    "    # Format doctor output as JSON string for the model to learn\n",
    "    doctor_response = json.dumps(item['doctor_output'], indent=2)\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a professional medical doctor. When a patient describes their symptoms, provide a structured prescription response in JSON format with: prescription_text, medicine_name, dose_size, frequency, duration, and speech (natural language explanation).\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": item['patient_input']\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": doctor_response\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return {\"conversation\": conversation}\n",
    "\n",
    "# Convert all data\n",
    "chat_data = [convert_to_chat_format(item) for item in processed_data]\n",
    "\n",
    "# Create Dataset\n",
    "dataset = Dataset.from_list(chat_data)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(\"First conversation:\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0890a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format conversations with chat template\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversation\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "print(\"Formatted prompt sample:\")\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Optimal training configuration for medical data\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,  # Lower for medical precision\n",
    "        gradient_accumulation_steps = 4,  # Effective batch size = 8\n",
    "        warmup_steps = 10,  # 10% of total steps (~50 examples)\n",
    "        num_train_epochs = 3,  # Multiple epochs for small dataset\n",
    "        learning_rate = 2e-5,  # Lower LR for medical accuracy\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",  # Cosine for smooth convergence\n",
    "        seed = 42,\n",
    "        output_dir = \"../models/clinic-chatbot\",\n",
    "        save_strategy = \"epoch\",  # Save after each epoch\n",
    "        save_total_limit = 2,  # Keep only 2 best checkpoints\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf369b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train only on model responses (not user inputs or system prompts)\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")\n",
    "\n",
    "print(\"Training dataset preview:\")\n",
    "print(f\"Total examples: {len(trainer.train_dataset)}\")\n",
    "print(f\"Sample input_ids length: {len(trainer.train_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74104835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify training setup - check what parts are being trained\n",
    "sample_idx = 10\n",
    "print(\"Full prompt:\")\n",
    "print(tokenizer.decode(trainer.train_dataset[sample_idx][\"input_ids\"]))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Only training on (labels != -100):\")\n",
    "print(tokenizer.decode([x if x != -100 else tokenizer.pad_token_id for x in trainer.train_dataset[sample_idx][\"labels\"]]).replace(tokenizer.pad_token, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b20be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"Total steps: ~{len(dataset) * 3 // (2 * 4)} steps (3 epochs, batch_size=2, grad_accum=4)\")\n",
    "print(\"Expected training time: 15-30 minutes depending on GPU\\n\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554cefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable fast inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def get_prescription(patient_symptoms, max_new_tokens=512, temperature=0.3):\n",
    "    \"\"\"Get prescription from fine-tuned doctor model\"\"\"\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a professional medical doctor. When a patient describes their symptoms, provide a structured prescription response in JSON format with: prescription_text, medicine_name, dose_size, frequency, duration, and speech (natural language explanation).\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": patient_symptoms\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,  # Lower temp for more consistent medical responses\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "    \n",
    "    try:\n",
    "        # Try to parse JSON response\n",
    "        prescription = json.loads(response)\n",
    "        return prescription\n",
    "    except:\n",
    "        # If not valid JSON, return as text\n",
    "        return {\"raw_response\": response}\n",
    "\n",
    "# Test with medical symptoms\n",
    "test_cases = [\n",
    "    \"I have severe headache and light sensitivity.\",\n",
    "    \"I have nausea and vomiting.\",\n",
    "    \"I have joint pain in my knees.\",\n",
    "    \"I have persistent cough with phlegm.\",\n",
    "    \"I have difficulty sleeping and anxiety.\",\n",
    "]\n",
    "\n",
    "print(\"üè• Testing Fine-tuned Clinic Chatbot\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "for symptom in test_cases:\n",
    "    print(f\"PATIENT: {symptom}\")\n",
    "    prescription = get_prescription(symptom)\n",
    "    print(f\"DOCTOR PRESCRIPTION:\")\n",
    "    print(json.dumps(prescription, indent=2))\n",
    "    \n",
    "    # Print speech field separately for clarity\n",
    "    if 'speech' in prescription:\n",
    "        print(f\"\\nüí¨ DOCTOR SAYS: {prescription['speech']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d24681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "print(\"üíæ Saving fine-tuned model...\")\n",
    "\n",
    "# Save LoRA adapter (small, ~100MB)\n",
    "model.save_pretrained(\"../models/clinic-chatbot-lora\")\n",
    "tokenizer.save_pretrained(\"../models/clinic-chatbot-lora\")\n",
    "\n",
    "print(\"‚úÖ LoRA adapter saved to ../models/clinic-chatbot-lora\")\n",
    "\n",
    "# Optional: Merge and save full model (larger, but standalone)\n",
    "# model.save_pretrained_merged(\"../models/clinic-chatbot-merged\", tokenizer, save_method=\"merged_16bit\")\n",
    "# print(\"‚úÖ Merged model saved to ../models/clinic-chatbot-merged\")\n",
    "\n",
    "# Optional: Export to GGUF for Ollama/llama.cpp\n",
    "# model.save_pretrained_gguf(\"../models/clinic-chatbot-gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "# print(\"‚úÖ GGUF model saved to ../models/clinic-chatbot-gguf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e24e8",
   "metadata": {},
   "source": [
    "## üìä Fine-tuning Parameters Summary\n",
    "\n",
    "### Model Configuration\n",
    "- **Model**: `google/gemma-2-9b-it` (Gemma 2, 9B parameters, instruction-tuned)\n",
    "- **Quantization**: 4-bit (QLoRA) for memory efficiency\n",
    "- **Max Sequence Length**: 1024 tokens (optimal for short medical prescriptions)\n",
    "\n",
    "### LoRA Configuration\n",
    "- **Rank (r)**: 16 - Balanced between quality and speed\n",
    "- **Alpha**: 16 - Standard scaling (typically = r)\n",
    "- **Dropout**: 0 - Optimized for Unsloth\n",
    "- **Target Modules**: All attention + MLP layers (q/k/v/o_proj, gate/up/down_proj)\n",
    "\n",
    "### Training Hyperparameters\n",
    "- **Batch Size**: 2 (per device)\n",
    "- **Gradient Accumulation**: 4 steps (effective batch size = 8)\n",
    "- **Learning Rate**: 2e-5 (lower for medical precision)\n",
    "- **Epochs**: 3 (small dataset needs multiple passes)\n",
    "- **Warmup Steps**: 10\n",
    "- **LR Scheduler**: Cosine (smooth convergence)\n",
    "- **Optimizer**: AdamW 8-bit\n",
    "\n",
    "### Dataset\n",
    "- **Size**: ~500 examples (100 unique cases with duplicates)\n",
    "- **Format**: Patient symptoms ‚Üí Doctor prescription (JSON with speech field)\n",
    "- **Output Structure**:\n",
    "  - `prescription_text`: Brief diagnosis\n",
    "  - `medicine_name`: Medication name\n",
    "  - `dose_size`: Dosage amount\n",
    "  - `frequency`: When to take\n",
    "  - `duration`: How long\n",
    "  - `speech`: Natural language explanation (NEW)\n",
    "\n",
    "### Why These Parameters?\n",
    "\n",
    "1. **Small Dataset (500 examples)**:\n",
    "   - Multiple epochs (3) to learn patterns\n",
    "   - Lower learning rate (2e-5) for stability\n",
    "   - Smaller batch size to prevent overfitting\n",
    "\n",
    "2. **Medical Domain**:\n",
    "   - Low temperature (0.3) for consistent outputs\n",
    "   - Structured JSON output format\n",
    "   - Training only on model responses (not user inputs)\n",
    "\n",
    "3. **Resource Efficiency**:\n",
    "   - 4-bit quantization reduces VRAM by 75%\n",
    "   - Gradient checkpointing saves memory\n",
    "   - LoRA rank 16 is efficient yet effective\n",
    "\n",
    "4. **Expected Results**:\n",
    "   - Training Loss: Should converge to ~0.5-1.0\n",
    "   - Training Time: ~15-30 minutes on modern GPU\n",
    "   - Model Size: ~100MB LoRA adapter\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
