{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üè• Medical Prescription Model - Inference Only\n",
        "\n",
        "This notebook loads the fine-tuned clinic chatbot model and provides an interface for generating medical prescriptions based on patient symptoms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "from unsloth import FastLanguageModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Fine-tuned Model\n",
        "\n",
        "Loading the LoRA adapter merged with the base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_seq_length = 1024\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"../models/clinic-chatbot-lora\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Enable fast inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Function\n",
        "\n",
        "Function to generate structured prescription responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Inference function ready!\n"
          ]
        }
      ],
      "source": [
        "def get_prescription(patient_symptoms, max_new_tokens=512, temperature=0.3, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generate prescription from patient symptoms.\n",
        "    \n",
        "    Args:\n",
        "        patient_symptoms (str): Description of patient symptoms\n",
        "        max_new_tokens (int): Maximum tokens to generate\n",
        "        temperature (float): Sampling temperature (lower = more deterministic)\n",
        "        top_p (float): Nucleus sampling parameter\n",
        "    \n",
        "    Returns:\n",
        "        dict: Structured prescription with medicine details and speech\n",
        "    \"\"\"\n",
        "    \n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a professional medical doctor. When a patient describes their symptoms, provide a structured prescription response in JSON format with: prescription_text, medicine_name, dose_size, frequency, duration, and speech (natural language explanation).\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": patient_symptoms\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
        "    \n",
        "    try:\n",
        "        prescription = json.loads(response)\n",
        "        return prescription\n",
        "    except json.JSONDecodeError:\n",
        "        return {\"raw_response\": response, \"error\": \"Failed to parse JSON response\"}\n",
        "\n",
        "print(\"‚úÖ Inference function ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Cases\n",
        "\n",
        "Run inference on multiple test cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_cases = [\n",
        "    \"I have severe headache and light sensitivity.\",\n",
        "    \"I have nausea and vomiting.\",\n",
        "    \"I have joint pain in my knees.\",\n",
        "    \"I have persistent cough with phlegm.\",\n",
        "    \"I have difficulty sleeping and anxiety.\",\n",
        "]\n",
        "\n",
        "print(\"üè• Testing Fine-tuned Clinic Chatbot\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "for symptom in test_cases:\n",
        "    print(f\"PATIENT: {symptom}\")\n",
        "    prescription = get_prescription(symptom)\n",
        "    print(f\"DOCTOR PRESCRIPTION:\")\n",
        "    print(json.dumps(prescription, indent=2))\n",
        "    \n",
        "    if 'speech' in prescription:\n",
        "        print(f\"\\nüí¨ DOCTOR SAYS: {prescription['speech']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Mode\n",
        "\n",
        "Enter your own symptoms to get prescription.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom symptom input\n",
        "custom_symptom = \"I have fever and body aches.\"\n",
        "\n",
        "print(f\"PATIENT SYMPTOM: {custom_symptom}\\n\")\n",
        "prescription = get_prescription(custom_symptom)\n",
        "\n",
        "print(\"PRESCRIPTION:\")\n",
        "print(json.dumps(prescription, indent=2))\n",
        "\n",
        "if 'speech' in prescription:\n",
        "    print(f\"\\nüí¨ DOCTOR SAYS:\\n{prescription['speech']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch_inference(symptoms_list, temperature=0.3):\n",
        "    \"\"\"\n",
        "    Process multiple symptoms and return all prescriptions.\n",
        "    \n",
        "    Args:\n",
        "        symptoms_list (list): List of symptom descriptions\n",
        "        temperature (float): Sampling temperature\n",
        "    \n",
        "    Returns:\n",
        "        list: List of prescription dictionaries\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for symptom in symptoms_list:\n",
        "        prescription = get_prescription(symptom, temperature=temperature)\n",
        "        results.append({\n",
        "            \"symptom\": symptom,\n",
        "            \"prescription\": prescription\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example batch processing\n",
        "batch_symptoms = [\n",
        "    \"I have chest pain and shortness of breath.\",\n",
        "    \"I have skin rash and itching.\",\n",
        "    \"I have stomach pain and diarrhea.\"\n",
        "]\n",
        "\n",
        "batch_results = batch_inference(batch_symptoms)\n",
        "\n",
        "print(\"üìã Batch Inference Results\\n\" + \"=\"*80 + \"\\n\")\n",
        "for result in batch_results:\n",
        "    print(f\"SYMPTOM: {result['symptom']}\")\n",
        "    print(f\"PRESCRIPTION: {json.dumps(result['prescription'], indent=2)}\")\n",
        "    print(\"-\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Results\n",
        "\n",
        "Save inference results to JSON file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def save_results(results, filename=None):\n",
        "    \"\"\"\n",
        "    Save inference results to JSON file.\n",
        "    \n",
        "    Args:\n",
        "        results (list): List of result dictionaries\n",
        "        filename (str): Output filename (optional)\n",
        "    \"\"\"\n",
        "    if filename is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"inference_results_{timestamp}.json\"\n",
        "    \n",
        "    output_path = os.path.join(os.path.dirname(os.getcwd()), 'results', filename)\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    \n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    \n",
        "    print(f\"‚úÖ Results saved to: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# Save batch results\n",
        "# save_results(batch_results)\n",
        "print(\"üíæ Export function ready. Uncomment to save results.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Model Information:\")\n",
        "print(f\"Base Model: unsloth/gemma-3-4b-it-unsloth-bnb-4bit\")\n",
        "print(f\"Fine-tuned Adapter: ../models/clinic-chatbot-lora\")\n",
        "print(f\"Max Sequence Length: {max_seq_length}\")\n",
        "print(f\"Quantization: 4-bit\")\n",
        "print(f\"Device: {model.device}\")\n",
        "print(f\"Training Data: 100 medical symptom-prescription pairs\")\n",
        "print(f\"Training Epochs: 3\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
