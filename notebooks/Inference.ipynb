{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Medical Prescription Model - Inference Only\n",
        "\n",
        "This notebook loads the fine-tuned clinic chatbot model and provides an interface for generating medical prescriptions based on patient symptoms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "from unsloth import FastLanguageModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Fine-tuned Model\n",
        "\n",
        "Loading the LoRA adapter merged with the base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_seq_length = 1024\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"../models/clinic-chatbot-lora\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Enable fast inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Function\n",
        "\n",
        "Function to generate structured prescription responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_prescription(patient_symptoms, max_new_tokens=512, temperature=0.3, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generate prescription from patient symptoms.\n",
        "    \n",
        "    Args:\n",
        "        patient_symptoms (str): Description of patient symptoms\n",
        "        max_new_tokens (int): Maximum tokens to generate\n",
        "        temperature (float): Sampling temperature (lower = more deterministic)\n",
        "        top_p (float): Nucleus sampling parameter\n",
        "    \n",
        "    Returns:\n",
        "        dict: Structured prescription with medicine details and speech\n",
        "    \"\"\"\n",
        "    \n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a professional medical doctor. When a patient describes their symptoms, provide a structured prescription response in JSON format with: prescription_text, medicine_name, dose_size, frequency, duration, and speech (natural language explanation).\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": patient_symptoms\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
        "    \n",
        "    try:\n",
        "        prescription = json.loads(response)\n",
        "        return prescription\n",
        "    except json.JSONDecodeError:\n",
        "        return {\"raw_response\": response, \"error\": \"Failed to parse JSON response\"}\n",
        "\n",
        "print(\"‚úÖ Inference function ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Cases\n",
        "\n",
        "Run inference on multiple test cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_cases = [\n",
        "    \"I have leg pains and my nails are coming off\",\n",
        "]\n",
        "\n",
        "print(\"üè• Testing Fine-tuned Clinic Chatbot\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "for symptom in test_cases:\n",
        "    print(f\"PATIENT: {symptom}\")\n",
        "    prescription = get_prescription(symptom)\n",
        "    print(f\"DOCTOR PRESCRIPTION:\")\n",
        "    print(json.dumps(prescription, indent=2))\n",
        "    \n",
        "    if 'speech' in prescription:\n",
        "        print(f\"\\nüí¨ DOCTOR SAYS: {prescription['speech']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradio Chat Interface\n",
        "\n",
        "Interactive chat interface for the medical chatbot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_interface(patient_symptoms, temperature, max_tokens):\n",
        "    \"\"\"\n",
        "    Gradio interface wrapper for the prescription model.\n",
        "    \"\"\"\n",
        "    if not patient_symptoms.strip():\n",
        "        return \"‚ö†Ô∏è Please describe your symptoms.\", \"{}\"\n",
        "    \n",
        "    # Get prescription from model\n",
        "    prescription = get_prescription(\n",
        "        patient_symptoms, \n",
        "        max_new_tokens=int(max_tokens),\n",
        "        temperature=float(temperature)\n",
        "    )\n",
        "    \n",
        "    # Format the response\n",
        "    if 'error' in prescription:\n",
        "        return f\"‚ö†Ô∏è Error: {prescription['error']}\", json.dumps(prescription, indent=2)\n",
        "    \n",
        "    # Extract speech for display\n",
        "    speech = prescription.get('speech', 'No speech generated')\n",
        "    \n",
        "    # Format prescription details nicely\n",
        "    prescription_display = f\"\"\"\n",
        "## üè• Doctor's Prescription\n",
        "\n",
        "**Diagnosis:** {prescription.get('prescription_text', 'N/A')}\n",
        "\n",
        "**Medicine:** {prescription.get('medicine_name', 'N/A')}\n",
        "\n",
        "**Dosage:** {prescription.get('dose_size', 'N/A')}\n",
        "\n",
        "**Frequency:** {prescription.get('frequency', 'N/A')}\n",
        "\n",
        "**Duration:** {prescription.get('duration', 'N/A')}\n",
        "\n",
        "---\n",
        "\n",
        "### üí¨ Doctor Says:\n",
        "{speech}\n",
        "\"\"\"\n",
        "    \n",
        "    # Return both the formatted display and JSON\n",
        "    return prescription_display, json.dumps(prescription, indent=2)\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"üè• Medical Chatbot\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üè• Medical Prescription Chatbot\n",
        "    \n",
        "    ### Describe your symptoms and receive a medical prescription.\n",
        "    \n",
        "    ‚ö†Ô∏è **Disclaimer:** This is a fine-tuned AI model for educational purposes only. \n",
        "    Always consult a real healthcare professional for medical advice.\n",
        "    \"\"\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            # Input area\n",
        "            symptoms_input = gr.Textbox(\n",
        "                label=\"üó£Ô∏è Patient Symptoms\",\n",
        "                placeholder=\"Describe your symptoms here (e.g., 'I have severe headache and nausea')\",\n",
        "                lines=3\n",
        "            )\n",
        "            \n",
        "            # Advanced settings\n",
        "            with gr.Accordion(\"‚öôÔ∏è Advanced Settings\", open=False):\n",
        "                temperature_slider = gr.Slider(\n",
        "                    minimum=0.1,\n",
        "                    maximum=1.0,\n",
        "                    value=0.3,\n",
        "                    step=0.1,\n",
        "                    label=\"Temperature (lower = more consistent)\",\n",
        "                )\n",
        "                max_tokens_slider = gr.Slider(\n",
        "                    minimum=128,\n",
        "                    maximum=1024,\n",
        "                    value=512,\n",
        "                    step=64,\n",
        "                    label=\"Max Tokens\",\n",
        "                )\n",
        "            \n",
        "            # Submit button\n",
        "            submit_btn = gr.Button(\"üíä Get Prescription\", variant=\"primary\", size=\"lg\")\n",
        "        \n",
        "        with gr.Column(scale=3):\n",
        "            # Output areas\n",
        "            prescription_output = gr.Markdown(label=\"üìã Prescription\")\n",
        "            \n",
        "            with gr.Accordion(\"üîç Raw JSON Output\", open=False):\n",
        "                json_output = gr.Code(label=\"JSON Response\", language=\"json\")\n",
        "    \n",
        "    # Example cases\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            [\"I have severe headache and light sensitivity.\", 0.3, 512],\n",
        "            [\"I have nausea and vomiting.\", 0.3, 512],\n",
        "            [\"I have joint pain in my knees.\", 0.3, 512],\n",
        "            [\"I have persistent cough with phlegm.\", 0.3, 512],\n",
        "            [\"I have leg pains and my nails are coming off.\", 0.3, 512],\n",
        "            [\"I have difficulty sleeping and anxiety.\", 0.3, 512],\n",
        "        ],\n",
        "        inputs=[symptoms_input, temperature_slider, max_tokens_slider],\n",
        "        label=\"üìù Example Symptoms\"\n",
        "    )\n",
        "    \n",
        "    # Connect the interface\n",
        "    submit_btn.click(\n",
        "        fn=chat_interface,\n",
        "        inputs=[symptoms_input, temperature_slider, max_tokens_slider],\n",
        "        outputs=[prescription_output, json_output]\n",
        "    )\n",
        "    \n",
        "    # Also allow Enter key to submit\n",
        "    symptoms_input.submit(\n",
        "        fn=chat_interface,\n",
        "        inputs=[symptoms_input, temperature_slider, max_tokens_slider],\n",
        "        outputs=[prescription_output, json_output]\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Gradio interface ready!\")\n",
        "print(\"Run: demo.launch() to start the chat interface\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Launch the Chat Interface\n",
        "\n",
        "Run this cell to start the interactive Gradio interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the Gradio interface\n",
        "# Use share=True to get a public link for 72 hours\n",
        "# Use share=False to run locally only\n",
        "\n",
        "demo.launch(\n",
        "    share=False,  # Set to True for public link\n",
        "    server_name=\"0.0.0.0\",  # Allow external access\n",
        "    server_port=7860,  # Default Gradio port\n",
        "    show_error=True,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
