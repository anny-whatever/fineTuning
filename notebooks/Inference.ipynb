{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Medical Prescription Model - Inference Only\n",
        "\n",
        "This notebook loads the fine-tuned clinic chatbot model and provides an interface for generating medical prescriptions based on patient symptoms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "from unsloth import FastLanguageModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Fine-tuned Model\n",
        "\n",
        "Loading the LoRA adapter merged with the base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_seq_length = 1024\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"../models/clinic-chatbot-lora\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Enable fast inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Function\n",
        "\n",
        "Function to generate structured prescription responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_prescription(patient_symptoms, max_new_tokens=512, temperature=0.3, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generate prescription from patient symptoms.\n",
        "    \n",
        "    Args:\n",
        "        patient_symptoms (str): Description of patient symptoms\n",
        "        max_new_tokens (int): Maximum tokens to generate\n",
        "        temperature (float): Sampling temperature (lower = more deterministic)\n",
        "        top_p (float): Nucleus sampling parameter\n",
        "    \n",
        "    Returns:\n",
        "        dict: Structured prescription with medicine details and speech\n",
        "    \"\"\"\n",
        "    \n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a professional medical doctor. When a patient describes their symptoms, provide a structured prescription response in JSON format with: prescription_text, medicine_name, dose_size, frequency, duration, and speech (natural language explanation).\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": patient_symptoms\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
        "    \n",
        "    try:\n",
        "        prescription = json.loads(response)\n",
        "        return prescription\n",
        "    except json.JSONDecodeError:\n",
        "        return {\"raw_response\": response, \"error\": \"Failed to parse JSON response\"}\n",
        "\n",
        "print(\"‚úÖ Inference function ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Cases\n",
        "\n",
        "Run inference on multiple test cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_cases = [\n",
        "    \"I have leg pains and my nails are coming off\",\n",
        "]\n",
        "\n",
        "print(\"üè• Testing Fine-tuned Clinic Chatbot\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "for symptom in test_cases:\n",
        "    print(f\"PATIENT: {symptom}\")\n",
        "    prescription = get_prescription(symptom)\n",
        "    print(f\"DOCTOR PRESCRIPTION:\")\n",
        "    print(json.dumps(prescription, indent=2))\n",
        "    \n",
        "    if 'speech' in prescription:\n",
        "        print(f\"\\nüí¨ DOCTOR SAYS: {prescription['speech']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
